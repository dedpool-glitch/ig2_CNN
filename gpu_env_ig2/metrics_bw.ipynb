{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook contains accuracy metrics for black-white groups across all emotions and individually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emonet import EmoNet\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmoNet(n_expression=8)\n",
    "model.load_state_dict(torch.load(\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\\\finetuned_emonet_100_epochs.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining image transform\n",
    "cfd_transform=transforms.Compose([ transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {'N': 0, 'A': 1, 'F': 2, 'HC': 3, 'HO': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    images_list = []\n",
    "    label_list = []\n",
    "    filenames = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    \n",
    "    for file in filenames:\n",
    "        img_path = os.path.join(path, file)\n",
    "        last_char = file[-5]\n",
    "        if last_char=='C' or last_char=='O':\n",
    "            last_char='H'+last_char\n",
    "        label = emotion_map[last_char]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = cfd_transform(image)\n",
    "        images_list.append(image)\n",
    "        label_list.append(label)\n",
    "\n",
    "    images_tensor = torch.stack(images_list) if images_list else torch.empty(0)\n",
    "    labels_tensor = torch.tensor(label_list, dtype=torch.long) if label_list else torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading black and white datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black image tensor shape: torch.Size([526, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "path_abm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Angry\"\n",
    "path_fbm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Fear\"\n",
    "path_nbm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\\\Neutral\"\n",
    "path_hcbm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Happy_ClosedMouth\"\n",
    "path_hobm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Happy_OpenMouth\"\n",
    "\n",
    "path_abf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Angry\"\n",
    "path_fbf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Fear\"\n",
    "path_nbf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\\\Neutral\"\n",
    "path_hcbf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Happy_ClosedMouth\"\n",
    "path_hobf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Happy_OpenMouth\"\n",
    "\n",
    "abm_list,abm_labels=load_images(path_abm)\n",
    "fbm_list,fbm_labels=load_images(path_fbm)\n",
    "nbm_list,nbm_labels=load_images(path_nbm)\n",
    "hcbm_list,hcbm_labels=load_images(path_hcbm)\n",
    "hobm_list,hobm_labels=load_images(path_hobm)\n",
    "\n",
    "abf_list,abf_labels=load_images(path_abf)\n",
    "fbf_list,fbf_labels=load_images(path_fbf)\n",
    "nbf_list,nbf_labels=load_images(path_nbf)\n",
    "hcbf_list,hcbf_labels=load_images(path_hcbf)\n",
    "hobf_list,hobf_labels=load_images(path_hobf)\n",
    "\n",
    "#we've loaded corresponding images, now stack everything to create one whole tensor for black people.\n",
    "\n",
    "black_images = torch.cat([\n",
    "    abm_list, fbm_list, nbm_list, hcbm_list, hobm_list, \n",
    "    abf_list, fbf_list, nbf_list, hcbf_list, hobf_list\n",
    "], dim=0)\n",
    "\n",
    "black_labels=torch.cat([\n",
    "abm_labels,fbm_labels,nbm_labels,hcbm_labels,hobm_labels,abf_labels,fbf_labels,nbf_labels,hcbf_labels,hobf_labels\n",
    "])\n",
    "\n",
    "print(\"Black image tensor shape:\", black_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white image tensor shape: torch.Size([464, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "path_awm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Angry\"\n",
    "path_fwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Fear\"\n",
    "path_nwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\\\Neutral\"\n",
    "path_hcwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Happy_ClosedMouth\"\n",
    "path_howm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Happy_OpenMouth\"\n",
    "\n",
    "path_awf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Angry\"\n",
    "path_fwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Fear\"\n",
    "path_nwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\\\Neutral\"\n",
    "path_hcwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Happy_ClosedMouth\"\n",
    "path_howf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Happy_OpenMouth\"\n",
    "\n",
    "awm_list,awm_labels=load_images(path_awm)\n",
    "fwm_list,fwm_labels=load_images(path_fwm)\n",
    "nwm_list,nwm_labels=load_images(path_nwm)\n",
    "hcwm_list,hcwm_labels=load_images(path_hcwm)\n",
    "howm_list,howm_labels=load_images(path_howm)\n",
    "\n",
    "awf_list,awf_labels=load_images(path_awf)\n",
    "fwf_list,fwf_labels=load_images(path_fwf)\n",
    "nwf_list,nwf_labels=load_images(path_nwf)\n",
    "hcwf_list,hcwf_labels=load_images(path_hcwf)\n",
    "howf_list,howf_labels=load_images(path_howf)\n",
    "\n",
    "\n",
    "white_images = torch.cat([\n",
    "    awm_list, fwm_list, nwm_list, hcwm_list, howm_list, \n",
    "    awf_list, fwf_list, nwf_list, hcwf_list, howf_list\n",
    "], dim=0)\n",
    "\n",
    "white_labels=torch.cat([\n",
    "awm_labels,fwm_labels,nwm_labels,hcwm_labels,howm_labels,awf_labels,fwf_labels,nwf_labels,hcwf_labels,howf_labels\n",
    "])\n",
    "\n",
    "print(\"white image tensor shape:\", white_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract relevant emotion logits, we only need 4 instead of 8\n",
    "def extract_relevant_logits(output):\n",
    "    logits=output[\"expression\"]\n",
    "    relevant_logits = torch.stack([\n",
    "        logits[:, 0],  # N\n",
    "        logits[:, 6],  # A\n",
    "        logits[:, 4],  # F\n",
    "        logits[:, 1],  # (HappyClosedMouth + HappyOpenMouth)\n",
    "    ], dim=1) \n",
    "\n",
    "    return relevant_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emonet(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = cfd_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_label = torch.argmax(extract_relevant_logits(output), dim=1).item()\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(image_tensor, label_tensor, model):\n",
    "    correct = 0\n",
    "    total = image_tensor.shape[0] \n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            image = image_tensor[i].unsqueeze(0).to(device)\n",
    "            true_label = label_tensor[i].item() \n",
    "            output = model(image)\n",
    "            pred_label = torch.argmax(extract_relevant_logits(output), dim=1).item()\n",
    "\n",
    "            if pred_label == true_label:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total * 100 if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for all black and white images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black imgs: 91.83%\n",
      "accuracy white imgs: 90.95%\n"
     ]
    }
   ],
   "source": [
    "black_accuracy = calculate_accuracy(black_images, black_labels, model)\n",
    "white_accuracy = calculate_accuracy(white_images, white_labels, model)\n",
    "\n",
    "print(f\"accuracy black imgs: {black_accuracy:.2f}%\")\n",
    "print(f\"accuracy white imgs: {white_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for all black and white angry images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black angry image tensor shape: torch.Size([82, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_angry_images = torch.cat([\n",
    "    abm_list, abf_list\n",
    "], dim=0)\n",
    "\n",
    "black_angry_labels=torch.cat([\n",
    "abm_labels,abf_labels\n",
    "])\n",
    "\n",
    "print(\"Black angry image tensor shape:\", black_angry_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white angry image tensor shape: torch.Size([72, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "white_angry_images = torch.cat([\n",
    "    awm_list, awf_list\n",
    "], dim=0)\n",
    "\n",
    "white_angry_labels=torch.cat([\n",
    "awm_labels,awf_labels\n",
    "])\n",
    "\n",
    "print(\"white angry image tensor shape:\", white_angry_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black angry imgs: 85.37%\n",
      "accuracy white angry imgs: 86.11%\n"
     ]
    }
   ],
   "source": [
    "black_angry_accuracy = calculate_accuracy(black_angry_images, black_angry_labels, model)\n",
    "white_angry_accuracy = calculate_accuracy(white_angry_images, white_angry_labels, model)\n",
    "\n",
    "print(f\"accuracy black angry imgs: {black_angry_accuracy:.2f}%\")\n",
    "print(f\"accuracy white angry imgs: {white_angry_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for angry b and w males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black angry male imgs: 91.43%\n",
      "accuracy white angry male imgs: 85.71%\n"
     ]
    }
   ],
   "source": [
    "black_angry_male_accuracy = calculate_accuracy(abm_list, abm_labels, model)\n",
    "white_angry_male_accuracy = calculate_accuracy(awm_list,awm_labels, model)\n",
    "\n",
    "print(f\"accuracy black angry male imgs: {black_angry_male_accuracy:.2f}%\")\n",
    "print(f\"accuracy white angry male imgs: {white_angry_male_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for angry b and w females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black angry female imgs: 80.85%\n",
      "accuracy white angry female imgs: 86.49%\n"
     ]
    }
   ],
   "source": [
    "black_angry_female_accuracy = calculate_accuracy(abf_list, abf_labels, model)\n",
    "white_angry_female_accuracy = calculate_accuracy(awf_list,awf_labels, model)\n",
    "\n",
    "print(f\"accuracy black angry female imgs: {black_angry_female_accuracy:.2f}%\")\n",
    "print(f\"accuracy white angry female imgs: {white_angry_female_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for all fear b and w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black fear image tensor shape: torch.Size([83, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_fear_images = torch.cat([\n",
    "    fbm_list, fbf_list\n",
    "], dim=0)\n",
    "\n",
    "black_fear_labels=torch.cat([\n",
    "fbm_labels,fbf_labels\n",
    "])\n",
    "\n",
    "print(\"Black fear image tensor shape:\", black_fear_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white fear image tensor shape: torch.Size([66, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "white_fear_images = torch.cat([\n",
    "    fwm_list, fwf_list\n",
    "], dim=0)\n",
    "\n",
    "white_fear_labels=torch.cat([\n",
    "fwm_labels,fwf_labels\n",
    "])\n",
    "\n",
    "print(\"white fear image tensor shape:\", white_fear_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black fear imgs: 90.36%\n",
      "accuracy white fear imgs: 81.82%\n"
     ]
    }
   ],
   "source": [
    "black_fear_accuracy = calculate_accuracy(black_fear_images, black_fear_labels, model)\n",
    "white_fear_accuracy = calculate_accuracy(white_fear_images, white_fear_labels, model)\n",
    "\n",
    "print(f\"accuracy black fear imgs: {black_fear_accuracy:.2f}%\")\n",
    "print(f\"accuracy white fear imgs: {white_fear_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for male fear b and w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black fear male imgs: 94.29%\n",
      "accuracy white fear male imgs: 89.66%\n"
     ]
    }
   ],
   "source": [
    "black_fear_male_accuracy = calculate_accuracy(fbm_list, fbm_labels, model)\n",
    "white_fear_male_accuracy = calculate_accuracy(fwm_list,fwm_labels, model)\n",
    "\n",
    "print(f\"accuracy black fear male imgs: {black_fear_male_accuracy:.2f}%\")\n",
    "print(f\"accuracy white fear male imgs: {white_fear_male_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for female fear b and w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black fear female imgs: 87.50%\n",
      "accuracy white fear female imgs: 75.68%\n"
     ]
    }
   ],
   "source": [
    "black_fear_female_accuracy = calculate_accuracy(fbf_list, fbf_labels, model)\n",
    "white_fear_female_accuracy = calculate_accuracy(fwf_list,fwf_labels, model)\n",
    "\n",
    "print(f\"accuracy black fear female imgs: {black_fear_female_accuracy:.2f}%\")\n",
    "print(f\"accuracy white fear female imgs: {white_fear_female_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all b-w neutral images accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black neutral image tensor shape: torch.Size([197, 3, 256, 256])\n",
      "White neutral image tensor shape: torch.Size([183, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_neutral_images = torch.cat([nbm_list, nbf_list], dim=0)\n",
    "black_neutral_labels = torch.cat([nbm_labels, nbf_labels])\n",
    "\n",
    "white_neutral_images = torch.cat([nwm_list, nwf_list], dim=0)\n",
    "white_neutral_labels = torch.cat([nwm_labels, nwf_labels])\n",
    "\n",
    "print(\"Black neutral image tensor shape:\", black_neutral_images.shape)\n",
    "print(\"White neutral image tensor shape:\", white_neutral_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black neutral imgs: 90.36%\n",
      "Accuracy white neutral imgs: 93.99%\n"
     ]
    }
   ],
   "source": [
    "black_neutral_accuracy = calculate_accuracy(black_neutral_images, black_neutral_labels, model)\n",
    "white_neutral_accuracy = calculate_accuracy(white_neutral_images, white_neutral_labels, model)\n",
    "\n",
    "print(f\"Accuracy black neutral imgs: {black_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white neutral imgs: {white_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for neutral male b-w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black male neutral imgs: 93.55%\n",
      "Accuracy white male neutral imgs: 96.77%\n"
     ]
    }
   ],
   "source": [
    "black_male_neutral_accuracy = calculate_accuracy(nbm_list, nbm_labels, model)\n",
    "white_male_neutral_accuracy = calculate_accuracy(nwm_list, nwm_labels, model)\n",
    "\n",
    "print(f\"Accuracy black male neutral imgs: {black_male_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white male neutral imgs: {white_male_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for neutral female b-w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black female neutral imgs: 87.50%\n",
      "Accuracy white female neutral imgs: 91.11%\n"
     ]
    }
   ],
   "source": [
    "black_female_neutral_accuracy = calculate_accuracy(nbf_list, nbf_labels, model)\n",
    "white_female_neutral_accuracy = calculate_accuracy(nwf_list, nwf_labels, model)\n",
    "\n",
    "print(f\"Accuracy black female neutral imgs: {black_female_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white female neutral imgs: {white_female_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for all b-w happy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black happy image tensor shape: torch.Size([164, 3, 256, 256])\n",
      "White happy image tensor shape: torch.Size([143, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_happy_images = torch.cat([hcbm_list, hcbf_list,hobf_list,hobm_list], dim=0)\n",
    "black_happy_labels = torch.cat([hcbm_labels, hcbf_labels,hobf_labels,hobm_labels])\n",
    "\n",
    "white_happy_images = torch.cat([hcwm_list, hcwf_list,howf_list,howm_list], dim=0)\n",
    "white_happy_labels = torch.cat([hcwm_labels, hcwf_labels,howf_labels,howm_labels])\n",
    "\n",
    "print(\"Black happy image tensor shape:\", black_happy_images.shape)\n",
    "print(\"White happy image tensor shape:\", white_happy_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black happy imgs: 97.56%\n",
      "Accuracy white happy imgs: 93.71%\n"
     ]
    }
   ],
   "source": [
    "black_happy_accuracy = calculate_accuracy(black_happy_images, black_happy_labels, model)\n",
    "white_happy_accuracy = calculate_accuracy(white_happy_images, white_happy_labels, model)\n",
    "\n",
    "print(f\"Accuracy black happy imgs: {black_happy_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white happy imgs: {white_happy_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for b-w male happy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black happy image tensor shape: torch.Size([68, 3, 256, 256])\n",
      "White happy image tensor shape: torch.Size([71, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_happymale_images = torch.cat([hcbm_list,hobm_list], dim=0)\n",
    "black_happymale_labels = torch.cat([hcbm_labels,hobm_labels])\n",
    "\n",
    "white_happymale_images = torch.cat([hcwm_list,howm_list], dim=0)\n",
    "white_happymale_labels = torch.cat([hcwm_labels,howm_labels])\n",
    "\n",
    "print(\"Black happy image tensor shape:\", black_happymale_images.shape)\n",
    "print(\"White happy image tensor shape:\", white_happymale_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black happy male imgs: 98.53%\n",
      "Accuracy white happy male imgs: 90.14%\n"
     ]
    }
   ],
   "source": [
    "black_happymale_accuracy = calculate_accuracy(black_happymale_images, black_happymale_labels, model)\n",
    "white_happymale_accuracy = calculate_accuracy(white_happymale_images, white_happymale_labels, model)\n",
    "\n",
    "print(f\"Accuracy black happy male imgs: {black_happymale_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white happy male imgs: {white_happymale_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for b-w female happy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black happy image tensor shape: torch.Size([96, 3, 256, 256])\n",
      "White happy image tensor shape: torch.Size([72, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_happyfemale_images = torch.cat([hcbf_list,hobf_list], dim=0)\n",
    "black_happyfemale_labels = torch.cat([hcbf_labels,hobf_labels])\n",
    "\n",
    "white_happyfemale_images = torch.cat([hcwf_list,howf_list], dim=0)\n",
    "white_happyfemale_labels = torch.cat([hcwf_labels,howf_labels])\n",
    "\n",
    "print(\"Black happy image tensor shape:\", black_happyfemale_images.shape)\n",
    "print(\"White happy image tensor shape:\", white_happyfemale_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black happy female imgs: 96.88%\n",
      "Accuracy white happy female imgs: 97.22%\n"
     ]
    }
   ],
   "source": [
    "black_happyfemale_accuracy = calculate_accuracy(black_happyfemale_images, black_happyfemale_labels, model)\n",
    "white_happyfemale_accuracy = calculate_accuracy(white_happyfemale_images, white_happyfemale_labels, model)\n",
    "\n",
    "print(f\"Accuracy black happy female imgs: {black_happyfemale_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white happy female imgs: {white_happyfemale_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list=[black_accuracy, white_accuracy,black_angry_accuracy,white_angry_accuracy,black_angry_male_accuracy,white_angry_male_accuracy,black_angry_female_accuracy,white_angry_female_accuracy,black_fear_accuracy,white_fear_accuracy,black_fear_male_accuracy,white_fear_male_accuracy,black_fear_female_accuracy,white_fear_female_accuracy,black_happy_accuracy,white_happy_accuracy,black_happymale_accuracy,white_happymale_accuracy,black_happyfemale_accuracy,white_happyfemale_accuracy,black_neutral_accuracy,white_neutral_accuracy,black_male_neutral_accuracy,white_male_neutral_accuracy,black_female_neutral_accuracy,white_female_neutral_accuracy]\n",
    "col_list = [\n",
    "    \"black_accuracy\", \"white_accuracy\", \"black_angry_accuracy\", \"white_angry_accuracy\",\n",
    "    \"black_angry_male_accuracy\", \"white_angry_male_accuracy\", \"black_angry_female_accuracy\", \"white_angry_female_accuracy\",\n",
    "    \"black_fear_accuracy\", \"white_fear_accuracy\", \"black_fear_male_accuracy\", \"white_fear_male_accuracy\",\n",
    "    \"black_fear_female_accuracy\", \"white_fear_female_accuracy\", \"black_happy_accuracy\", \"white_happy_accuracy\",\n",
    "    \"black_happymale_accuracy\", \"white_happymale_accuracy\", \"black_happyfemale_accuracy\", \"white_happyfemale_accuracy\",\n",
    "    \"black_neutral_accuracy\", \"white_neutral_accuracy\", \"black_male_neutral_accuracy\", \"white_male_neutral_accuracy\",\n",
    "    \"black_female_neutral_accuracy\", \"white_female_neutral_accuracy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_accuracy = [round(value, 3) for value in accuracy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>black_accuracy</th>\n",
       "      <td>91.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_accuracy</th>\n",
       "      <td>90.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_angry_accuracy</th>\n",
       "      <td>85.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_angry_accuracy</th>\n",
       "      <td>86.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_angry_male_accuracy</th>\n",
       "      <td>91.429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy(%)\n",
       "black_accuracy                  91.825\n",
       "white_accuracy                  90.948\n",
       "black_angry_accuracy            85.366\n",
       "white_angry_accuracy            86.111\n",
       "black_angry_male_accuracy       91.429"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(rounded_accuracy,col_list,columns=['accuracy(%)'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('metrics_bw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
