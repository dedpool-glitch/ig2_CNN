{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook contains accuracy metrics for black-white groups across all emotions and individually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emonet import EmoNet\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmoNet(n_expression=8)\n",
    "model.load_state_dict(torch.load(\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\\\cfd_finetuned_emonet_100_epochs.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining image transform\n",
    "cfd_transform=transforms.Compose([ transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {'N': 0, 'A': 1, 'F': 2, 'HC': 3, 'HO': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    images_list = []\n",
    "    label_list = []\n",
    "    filenames = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    \n",
    "    for file in filenames:\n",
    "        img_path = os.path.join(path, file)\n",
    "        last_char = file[-5]\n",
    "        if last_char=='C' or last_char=='O':\n",
    "            last_char='H'+last_char\n",
    "        label = emotion_map[last_char]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = cfd_transform(image)\n",
    "        images_list.append(image)\n",
    "        label_list.append(label)\n",
    "\n",
    "    images_tensor = torch.stack(images_list) if images_list else torch.empty(0)\n",
    "    labels_tensor = torch.tensor(label_list, dtype=torch.long) if label_list else torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading black and white datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black image tensor shape: torch.Size([526, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "path_abm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Angry\"\n",
    "path_fbm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Fear\"\n",
    "path_nbm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\\\Neutral\"\n",
    "path_hcbm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Happy_ClosedMouth\"\n",
    "path_hobm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Black\\Happy_OpenMouth\"\n",
    "\n",
    "path_abf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Angry\"\n",
    "path_fbf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Fear\"\n",
    "path_nbf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\\\Neutral\"\n",
    "path_hcbf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Happy_ClosedMouth\"\n",
    "path_hobf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Black\\Happy_OpenMouth\"\n",
    "\n",
    "abm_list,abm_labels=load_images(path_abm)\n",
    "fbm_list,fbm_labels=load_images(path_fbm)\n",
    "nbm_list,nbm_labels=load_images(path_nbm)\n",
    "hcbm_list,hcbm_labels=load_images(path_hcbm)\n",
    "hobm_list,hobm_labels=load_images(path_hobm)\n",
    "\n",
    "abf_list,abf_labels=load_images(path_abf)\n",
    "fbf_list,fbf_labels=load_images(path_fbf)\n",
    "nbf_list,nbf_labels=load_images(path_nbf)\n",
    "hcbf_list,hcbf_labels=load_images(path_hcbf)\n",
    "hobf_list,hobf_labels=load_images(path_hobf)\n",
    "\n",
    "#we've loaded corresponding images, now stack everything to create one whole tensor for black people.\n",
    "\n",
    "black_images = torch.cat([\n",
    "    abm_list, fbm_list, nbm_list, hcbm_list, hobm_list, \n",
    "    abf_list, fbf_list, nbf_list, hcbf_list, hobf_list\n",
    "], dim=0)\n",
    "\n",
    "black_labels=torch.cat([\n",
    "abm_labels,fbm_labels,nbm_labels,hcbm_labels,hobm_labels,abf_labels,fbf_labels,nbf_labels,hcbf_labels,hobf_labels\n",
    "])\n",
    "\n",
    "print(\"Black image tensor shape:\", black_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white image tensor shape: torch.Size([464, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "path_awm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Angry\"\n",
    "path_fwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Fear\"\n",
    "path_nwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\\\Neutral\"\n",
    "path_hcwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Happy_ClosedMouth\"\n",
    "path_howm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\Happy_OpenMouth\"\n",
    "\n",
    "path_awf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Angry\"\n",
    "path_fwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Fear\"\n",
    "path_nwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\\\Neutral\"\n",
    "path_hcwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Happy_ClosedMouth\"\n",
    "path_howf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\Happy_OpenMouth\"\n",
    "\n",
    "awm_list,awm_labels=load_images(path_awm)\n",
    "fwm_list,fwm_labels=load_images(path_fwm)\n",
    "nwm_list,nwm_labels=load_images(path_nwm)\n",
    "hcwm_list,hcwm_labels=load_images(path_hcwm)\n",
    "howm_list,howm_labels=load_images(path_howm)\n",
    "\n",
    "awf_list,awf_labels=load_images(path_awf)\n",
    "fwf_list,fwf_labels=load_images(path_fwf)\n",
    "nwf_list,nwf_labels=load_images(path_nwf)\n",
    "hcwf_list,hcwf_labels=load_images(path_hcwf)\n",
    "howf_list,howf_labels=load_images(path_howf)\n",
    "\n",
    "\n",
    "white_images = torch.cat([\n",
    "    awm_list, fwm_list, nwm_list, hcwm_list, howm_list, \n",
    "    awf_list, fwf_list, nwf_list, hcwf_list, howf_list\n",
    "], dim=0)\n",
    "\n",
    "white_labels=torch.cat([\n",
    "awm_labels,fwm_labels,nwm_labels,hcwm_labels,howm_labels,awf_labels,fwf_labels,nwf_labels,hcwf_labels,howf_labels\n",
    "])\n",
    "\n",
    "print(\"white image tensor shape:\", white_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract relevant emotion logits, we only need 4 instead of 8\n",
    "def extract_relevant_logits(output):\n",
    "    logits=output[\"expression\"]\n",
    "    relevant_logits = torch.stack([\n",
    "        logits[:, 0],  # N\n",
    "        logits[:, 6],  # A\n",
    "        logits[:, 4],  # F\n",
    "        logits[:, 1],  # (HappyClosedMouth + HappyOpenMouth)\n",
    "    ], dim=1) \n",
    "\n",
    "    return relevant_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emonet(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = cfd_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_label = torch.argmax(extract_relevant_logits(output), dim=1).item()\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(image_tensor, label_tensor, model):\n",
    "    correct = 0\n",
    "    total = image_tensor.shape[0] \n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            image = image_tensor[i].unsqueeze(0).to(device)\n",
    "            true_label = label_tensor[i].item() \n",
    "            output = model(image)\n",
    "            pred_label = torch.argmax(extract_relevant_logits(output), dim=1).item()\n",
    "\n",
    "            if pred_label == true_label:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total * 100 if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for all black and white images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black imgs: 91.83%\n",
      "accuracy white imgs: 90.95%\n"
     ]
    }
   ],
   "source": [
    "black_accuracy = calculate_accuracy(black_images, black_labels, model)\n",
    "white_accuracy = calculate_accuracy(white_images, white_labels, model)\n",
    "\n",
    "print(f\"accuracy black imgs: {black_accuracy:.2f}%\")\n",
    "print(f\"accuracy white imgs: {white_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for all black and white angry images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black angry image tensor shape: torch.Size([82, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_angry_images = torch.cat([\n",
    "    abm_list, abf_list\n",
    "], dim=0)\n",
    "\n",
    "black_angry_labels=torch.cat([\n",
    "abm_labels,abf_labels\n",
    "])\n",
    "\n",
    "print(\"Black angry image tensor shape:\", black_angry_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white angry image tensor shape: torch.Size([72, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "white_angry_images = torch.cat([\n",
    "    awm_list, awf_list\n",
    "], dim=0)\n",
    "\n",
    "white_angry_labels=torch.cat([\n",
    "awm_labels,awf_labels\n",
    "])\n",
    "\n",
    "print(\"white angry image tensor shape:\", white_angry_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black angry imgs: 85.37%\n",
      "accuracy white angry imgs: 86.11%\n"
     ]
    }
   ],
   "source": [
    "black_angry_accuracy = calculate_accuracy(black_angry_images, black_angry_labels, model)\n",
    "white_angry_accuracy = calculate_accuracy(white_angry_images, white_angry_labels, model)\n",
    "\n",
    "print(f\"accuracy black angry imgs: {black_angry_accuracy:.2f}%\")\n",
    "print(f\"accuracy white angry imgs: {white_angry_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for angry b and w males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black angry male imgs: 91.43%\n",
      "accuracy white angry male imgs: 85.71%\n"
     ]
    }
   ],
   "source": [
    "black_angry_male_accuracy = calculate_accuracy(abm_list, abm_labels, model)\n",
    "white_angry_male_accuracy = calculate_accuracy(awm_list,awm_labels, model)\n",
    "\n",
    "print(f\"accuracy black angry male imgs: {black_angry_male_accuracy:.2f}%\")\n",
    "print(f\"accuracy white angry male imgs: {white_angry_male_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for angry b and w females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black angry female imgs: 80.85%\n",
      "accuracy white angry female imgs: 86.49%\n"
     ]
    }
   ],
   "source": [
    "black_angry_female_accuracy = calculate_accuracy(abf_list, abf_labels, model)\n",
    "white_angry_female_accuracy = calculate_accuracy(awf_list,awf_labels, model)\n",
    "\n",
    "print(f\"accuracy black angry female imgs: {black_angry_female_accuracy:.2f}%\")\n",
    "print(f\"accuracy white angry female imgs: {white_angry_female_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for all fear b and w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black fear image tensor shape: torch.Size([83, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_fear_images = torch.cat([\n",
    "    fbm_list, fbf_list\n",
    "], dim=0)\n",
    "\n",
    "black_fear_labels=torch.cat([\n",
    "fbm_labels,fbf_labels\n",
    "])\n",
    "\n",
    "print(\"Black fear image tensor shape:\", black_fear_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbf_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white fear image tensor shape: torch.Size([66, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "white_fear_images = torch.cat([\n",
    "    fwm_list, fwf_list\n",
    "], dim=0)\n",
    "\n",
    "white_fear_labels=torch.cat([\n",
    "fwm_labels,fwf_labels\n",
    "])\n",
    "\n",
    "print(\"white fear image tensor shape:\", white_fear_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black fear imgs: 90.36%\n",
      "accuracy white fear imgs: 81.82%\n"
     ]
    }
   ],
   "source": [
    "black_fear_accuracy = calculate_accuracy(black_fear_images, black_fear_labels, model)\n",
    "white_fear_accuracy = calculate_accuracy(white_fear_images, white_fear_labels, model)\n",
    "\n",
    "print(f\"accuracy black fear imgs: {black_fear_accuracy:.2f}%\")\n",
    "print(f\"accuracy white fear imgs: {white_fear_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for male fear b and w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black fear male imgs: 94.29%\n",
      "accuracy white fear male imgs: 89.66%\n"
     ]
    }
   ],
   "source": [
    "black_fear_male_accuracy = calculate_accuracy(fbm_list, fbm_labels, model)\n",
    "white_fear_male_accuracy = calculate_accuracy(fwm_list,fwm_labels, model)\n",
    "\n",
    "print(f\"accuracy black fear male imgs: {black_fear_male_accuracy:.2f}%\")\n",
    "print(f\"accuracy white fear male imgs: {white_fear_male_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy calculation for female fear b and w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy black fear female imgs: 87.50%\n",
      "accuracy white fear female imgs: 75.68%\n"
     ]
    }
   ],
   "source": [
    "black_fear_female_accuracy = calculate_accuracy(fbf_list, fbf_labels, model)\n",
    "white_fear_female_accuracy = calculate_accuracy(fwf_list,fwf_labels, model)\n",
    "\n",
    "print(f\"accuracy black fear female imgs: {black_fear_female_accuracy:.2f}%\")\n",
    "print(f\"accuracy white fear female imgs: {white_fear_female_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all b-w neutral images accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black neutral image tensor shape: torch.Size([197, 3, 256, 256])\n",
      "White neutral image tensor shape: torch.Size([183, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_neutral_images = torch.cat([nbm_list, nbf_list], dim=0)\n",
    "black_neutral_labels = torch.cat([nbm_labels, nbf_labels])\n",
    "\n",
    "white_neutral_images = torch.cat([nwm_list, nwf_list], dim=0)\n",
    "white_neutral_labels = torch.cat([nwm_labels, nwf_labels])\n",
    "\n",
    "print(\"Black neutral image tensor shape:\", black_neutral_images.shape)\n",
    "print(\"White neutral image tensor shape:\", white_neutral_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black neutral imgs: 90.36%\n",
      "Accuracy white neutral imgs: 93.99%\n"
     ]
    }
   ],
   "source": [
    "black_neutral_accuracy = calculate_accuracy(black_neutral_images, black_neutral_labels, model)\n",
    "white_neutral_accuracy = calculate_accuracy(white_neutral_images, white_neutral_labels, model)\n",
    "\n",
    "print(f\"Accuracy black neutral imgs: {black_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white neutral imgs: {white_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for neutral male b-w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black male neutral imgs: 93.55%\n",
      "Accuracy white male neutral imgs: 96.77%\n"
     ]
    }
   ],
   "source": [
    "black_male_neutral_accuracy = calculate_accuracy(nbm_list, nbm_labels, model)\n",
    "white_male_neutral_accuracy = calculate_accuracy(nwm_list, nwm_labels, model)\n",
    "\n",
    "print(f\"Accuracy black male neutral imgs: {black_male_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white male neutral imgs: {white_male_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for neutral female b-w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black female neutral imgs: 87.50%\n",
      "Accuracy white female neutral imgs: 91.11%\n"
     ]
    }
   ],
   "source": [
    "black_female_neutral_accuracy = calculate_accuracy(nbf_list, nbf_labels, model)\n",
    "white_female_neutral_accuracy = calculate_accuracy(nwf_list, nwf_labels, model)\n",
    "\n",
    "print(f\"Accuracy black female neutral imgs: {black_female_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white female neutral imgs: {white_female_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for all b-w happy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black happy image tensor shape: torch.Size([164, 3, 256, 256])\n",
      "White happy image tensor shape: torch.Size([143, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_happy_images = torch.cat([hcbm_list, hcbf_list,hobf_list,hobm_list], dim=0)\n",
    "black_happy_labels = torch.cat([hcbm_labels, hcbf_labels,hobf_labels,hobm_labels])\n",
    "\n",
    "white_happy_images = torch.cat([hcwm_list, hcwf_list,howf_list,howm_list], dim=0)\n",
    "white_happy_labels = torch.cat([hcwm_labels, hcwf_labels,howf_labels,howm_labels])\n",
    "\n",
    "print(\"Black happy image tensor shape:\", black_happy_images.shape)\n",
    "print(\"White happy image tensor shape:\", white_happy_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black happy imgs: 97.56%\n",
      "Accuracy white happy imgs: 93.71%\n"
     ]
    }
   ],
   "source": [
    "black_happy_accuracy = calculate_accuracy(black_happy_images, black_happy_labels, model)\n",
    "white_happy_accuracy = calculate_accuracy(white_happy_images, white_happy_labels, model)\n",
    "\n",
    "print(f\"Accuracy black happy imgs: {black_happy_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white happy imgs: {white_happy_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for b-w male happy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black happy image tensor shape: torch.Size([68, 3, 256, 256])\n",
      "White happy image tensor shape: torch.Size([71, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_happymale_images = torch.cat([hcbm_list,hobm_list], dim=0)\n",
    "black_happymale_labels = torch.cat([hcbm_labels,hobm_labels])\n",
    "\n",
    "white_happymale_images = torch.cat([hcwm_list,howm_list], dim=0)\n",
    "white_happymale_labels = torch.cat([hcwm_labels,howm_labels])\n",
    "\n",
    "print(\"Black happy image tensor shape:\", black_happymale_images.shape)\n",
    "print(\"White happy image tensor shape:\", white_happymale_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black happy male imgs: 98.53%\n",
      "Accuracy white happy male imgs: 90.14%\n"
     ]
    }
   ],
   "source": [
    "black_happymale_accuracy = calculate_accuracy(black_happymale_images, black_happymale_labels, model)\n",
    "white_happymale_accuracy = calculate_accuracy(white_happymale_images, white_happymale_labels, model)\n",
    "\n",
    "print(f\"Accuracy black happy male imgs: {black_happymale_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white happy male imgs: {white_happymale_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for b-w female happy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black happy image tensor shape: torch.Size([96, 3, 256, 256])\n",
      "White happy image tensor shape: torch.Size([72, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "black_happyfemale_images = torch.cat([hcbf_list,hobf_list], dim=0)\n",
    "black_happyfemale_labels = torch.cat([hcbf_labels,hobf_labels])\n",
    "\n",
    "white_happyfemale_images = torch.cat([hcwf_list,howf_list], dim=0)\n",
    "white_happyfemale_labels = torch.cat([hcwf_labels,howf_labels])\n",
    "\n",
    "print(\"Black happy image tensor shape:\", black_happyfemale_images.shape)\n",
    "print(\"White happy image tensor shape:\", white_happyfemale_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy black happy female imgs: 96.88%\n",
      "Accuracy white happy female imgs: 97.22%\n"
     ]
    }
   ],
   "source": [
    "black_happyfemale_accuracy = calculate_accuracy(black_happyfemale_images, black_happyfemale_labels, model)\n",
    "white_happyfemale_accuracy = calculate_accuracy(white_happyfemale_images, white_happyfemale_labels, model)\n",
    "\n",
    "print(f\"Accuracy black happy female imgs: {black_happyfemale_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white happy female imgs: {white_happyfemale_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list=[black_accuracy, white_accuracy,black_angry_accuracy,white_angry_accuracy,black_angry_male_accuracy,white_angry_male_accuracy,black_angry_female_accuracy,white_angry_female_accuracy,black_fear_accuracy,white_fear_accuracy,black_fear_male_accuracy,white_fear_male_accuracy,black_fear_female_accuracy,white_fear_female_accuracy,black_happy_accuracy,white_happy_accuracy,black_happymale_accuracy,white_happymale_accuracy,black_happyfemale_accuracy,white_happyfemale_accuracy,black_neutral_accuracy,white_neutral_accuracy,black_male_neutral_accuracy,white_male_neutral_accuracy,black_female_neutral_accuracy,white_female_neutral_accuracy]\n",
    "col_list = [\n",
    "    \"black_accuracy\", \"white_accuracy\", \"black_angry_accuracy\", \"white_angry_accuracy\",\n",
    "    \"black_angry_male_accuracy\", \"white_angry_male_accuracy\", \"black_angry_female_accuracy\", \"white_angry_female_accuracy\",\n",
    "    \"black_fear_accuracy\", \"white_fear_accuracy\", \"black_fear_male_accuracy\", \"white_fear_male_accuracy\",\n",
    "    \"black_fear_female_accuracy\", \"white_fear_female_accuracy\", \"black_happy_accuracy\", \"white_happy_accuracy\",\n",
    "    \"black_happymale_accuracy\", \"white_happymale_accuracy\", \"black_happyfemale_accuracy\", \"white_happyfemale_accuracy\",\n",
    "    \"black_neutral_accuracy\", \"white_neutral_accuracy\", \"black_male_neutral_accuracy\", \"white_male_neutral_accuracy\",\n",
    "    \"black_female_neutral_accuracy\", \"white_female_neutral_accuracy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_accuracy = [round(value, 3) for value in accuracy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>black_accuracy</th>\n",
       "      <td>91.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_accuracy</th>\n",
       "      <td>90.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_angry_accuracy</th>\n",
       "      <td>85.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_angry_accuracy</th>\n",
       "      <td>86.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black_angry_male_accuracy</th>\n",
       "      <td>91.429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy(%)\n",
       "black_accuracy                  91.825\n",
       "white_accuracy                  90.948\n",
       "black_angry_accuracy            85.366\n",
       "white_angry_accuracy            86.111\n",
       "black_angry_male_accuracy       91.429"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(rounded_accuracy,col_list,columns=['accuracy(%)'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('metrics_bw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAIRNESS METRICS PRE-SUPPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def get_preds_in_batches_reduced(model, images, batch_size=32, device=\"cuda\"):\n",
    "    dataset = TensorDataset(images)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    preds = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "            logits = extract_relevant_logits(output)\n",
    "            pred = logits.argmax(dim=1).cpu()\n",
    "            preds.append(pred)\n",
    "\n",
    "    return torch.cat(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_metrics(preds, labels, target_class):\n",
    "    # True Positive Rate (TPR): correctly predicted as class y / all actual class y\n",
    "    # False Positive Rate (FPR): predicted as class y but actual not y / all actual not y\n",
    "    true_positive = ((preds == target_class) & (labels == target_class)).sum().item()\n",
    "    false_positive = ((preds == target_class) & (labels != target_class)).sum().item()\n",
    "    actual_positive = (labels == target_class).sum().item()\n",
    "    actual_negative = (labels != target_class).sum().item()\n",
    "\n",
    "    tpr = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    fpr = false_positive / actual_negative if actual_negative > 0 else 0\n",
    "\n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_equalized_odds_gap(model, black_images, black_labels, white_images, white_labels, target_class, device=\"cuda\"):\n",
    "    preds_b = get_preds_in_batches_reduced(model, black_images, device=device)\n",
    "    preds_w = get_preds_in_batches_reduced(model, white_images, device=device)\n",
    "\n",
    "    tpr_b, fpr_b = compute_group_metrics(preds_b, black_labels, target_class)\n",
    "    tpr_w, fpr_w = compute_group_metrics(preds_w, white_labels, target_class)\n",
    "\n",
    "    gap = abs(tpr_b - tpr_w) + abs(fpr_b - fpr_w)\n",
    "    return gap, tpr_b,tpr_w,fpr_b,fpr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Gap (Angry): 0.027\n"
     ]
    }
   ],
   "source": [
    "target_class = 1  # Angry is index 1 in our modified emotion set\n",
    "gap1,tpr_b1,tpr_w1,fpr_b1,fpr_w1 = compute_equalized_odds_gap(model, black_images, black_labels, white_images, white_labels, target_class)\n",
    "print(f\"Equalized Odds Gap (Angry): {gap1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR Black Angry: 0.854, TPR White Angry: 0.861\n",
      "FPR Black Angry: 0.014, FPR White Angry: 0.033\n"
     ]
    }
   ],
   "source": [
    "print(f\"TPR Black Angry: {tpr_b1:.3f}, TPR White Angry: {tpr_w1:.3f}\")\n",
    "print(f\"FPR Black Angry: {fpr_b1:.3f}, FPR White Angry: {fpr_w1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as observed above, the TPR for black angry and white angry images is almost similar and White individuals are more likely to be incorrectly predicted as angry, but the gap is small. therefore we observe close to no ethnic bias in the anger emotion for these 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Gap (Fear): 0.114\n"
     ]
    }
   ],
   "source": [
    "target_class = 2 #Fear is index 2\n",
    "gap2,tpr_b2,tpr_w2,fpr_b2,fpr_w2 = compute_equalized_odds_gap(model, black_images, black_labels, white_images, white_labels, target_class)\n",
    "print(f\"Equalized Odds Gap (Fear): {gap2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR Black Fear: 0.904, TPR White Fear: 0.818\n",
      "FPR Black Fear: 0.036, FPR White Fear: 0.008\n"
     ]
    }
   ],
   "source": [
    "print(f\"TPR Black Fear: {tpr_b2:.3f}, TPR White Fear: {tpr_w2:.3f}\")\n",
    "print(f\"FPR Black Fear: {fpr_b2:.3f}, FPR White Fear: {fpr_w2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for fear emotion across the 2 ethnicities, we observe that FPR in black images is ~ 4.5 times higher than for whites, which means more black images have been categorized as showing fear even when there is none, this could indicate some ethnic bias, as it predicts more black faces showing fear when there isn't any such thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Gap (Happy): 0.045\n"
     ]
    }
   ],
   "source": [
    "target_class = 3 #Happy is index 3\n",
    "gap3,tpr_b3,tpr_w3,fpr_b3,fpr_w3 = compute_equalized_odds_gap(model, black_images, black_labels, white_images, white_labels, target_class)\n",
    "print(f\"Equalized Odds Gap (Happy): {gap3:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR Black Happy: 0.976, TPR White Happy: 0.937\n",
      "FPR Black Happy: 0.022, FPR White Happy: 0.016\n"
     ]
    }
   ],
   "source": [
    "print(f\"TPR Black Happy: {tpr_b3:.3f}, TPR White Happy: {tpr_w3:.3f}\")\n",
    "print(f\"FPR Black Happy: {fpr_b3:.3f}, FPR White Happy: {fpr_w3:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Gap (Neutral): 0.072\n"
     ]
    }
   ],
   "source": [
    "target_class = 0 #Neutral is index 0\n",
    "gap0,tpr_b0,tpr_w0,fpr_b0,fpr_w0 = compute_equalized_odds_gap(model, black_images, black_labels, white_images, white_labels, target_class)\n",
    "print(f\"Equalized Odds Gap (Neutral): {gap0:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR Black N: 0.976, TPR White N: 0.937\n",
      "FPR Black N: 0.022, FPR White N: 0.016\n"
     ]
    }
   ],
   "source": [
    "print(f\"TPR Black N: {tpr_b3:.3f}, TPR White N: {tpr_w3:.3f}\")\n",
    "print(f\"FPR Black N: {fpr_b3:.3f}, FPR White N: {fpr_w3:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
