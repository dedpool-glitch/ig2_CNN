{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook contains accuracy metrics for asian american-white groups across all emotions and individually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emonet import EmoNet\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmoNet(n_expression=8)\n",
    "model.load_state_dict(torch.load(\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\\\cfd_finetuned_emonet_100_epochs.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining image transform\n",
    "cfd_transform=transforms.Compose([ transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {'N': 0, 'A': 1, 'F': 2, 'HC': 3, 'HO': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    images_list = []\n",
    "    label_list = []\n",
    "    filenames = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    \n",
    "    for file in filenames:\n",
    "        img_path = os.path.join(path, file)\n",
    "        last_char = file[-5]\n",
    "        if last_char=='C' or last_char=='O':\n",
    "            last_char='H'+last_char\n",
    "        label = emotion_map[last_char]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = cfd_transform(image)\n",
    "        images_list.append(image)\n",
    "        label_list.append(label)\n",
    "\n",
    "    images_tensor = torch.stack(images_list) if images_list else torch.empty(0)\n",
    "    labels_tensor = torch.tensor(label_list, dtype=torch.long) if label_list else torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading asian-american and white datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asian american image tensor shape: torch.Size([109, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "path_nam=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\Asian american\\\\Neutral\"\n",
    "path_naf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\Asian american\\\\Neutral\"\n",
    "\n",
    "\n",
    "nam_list,nam_labels=load_images(path_nam)\n",
    "naf_list,naf_labels=load_images(path_naf)\n",
    "\n",
    "\n",
    "#we've loaded corresponding images, now stack everything to create one whole tensor for asian american people.\n",
    "\n",
    "asian_american_images = torch.cat([\n",
    "    nam_list,naf_list\n",
    "], dim=0)\n",
    "\n",
    "asian_american_labels=torch.cat([\n",
    "nam_labels,naf_labels\n",
    "])\n",
    "\n",
    "print(\"Asian american image tensor shape:\", asian_american_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white image tensor shape: torch.Size([183, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "path_nwm=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Male\\White\\\\Neutral\"\n",
    "path_nwf=\"D:\\Integrated_gap_gradients\\ig2_CNN\\gpu_env_ig2\\Female\\White\\\\Neutral\"\n",
    "\n",
    "nwm_list,nwm_labels=load_images(path_nwm)\n",
    "nwf_list,nwf_labels=load_images(path_nwf)\n",
    "\n",
    "\n",
    "\n",
    "white_images = torch.cat([\n",
    "    nwm_list,nwf_list\n",
    "], dim=0)\n",
    "\n",
    "white_labels=torch.cat([\n",
    "nwm_labels,nwf_labels\n",
    "])\n",
    "\n",
    "print(\"white image tensor shape:\", white_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract relevant emotion logits, we only need 4 instead of 8\n",
    "def extract_relevant_logits(output):\n",
    "    logits=output[\"expression\"]\n",
    "    relevant_logits = torch.stack([\n",
    "        logits[:, 0],  # N\n",
    "        logits[:, 6],  # A\n",
    "        logits[:, 4],  # F\n",
    "        logits[:, 1],  # (HappyClosedMouth + HappyOpenMouth)\n",
    "    ], dim=1) \n",
    "\n",
    "    return relevant_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emonet(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = cfd_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_label = torch.argmax(extract_relevant_logits(output), dim=1).item()\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(image_tensor, label_tensor, model):\n",
    "    correct = 0\n",
    "    total = image_tensor.shape[0] \n",
    "    with torch.no_grad():\n",
    "        for i in range(total):\n",
    "            image = image_tensor[i].unsqueeze(0).to(device)\n",
    "            true_label = label_tensor[i].item() \n",
    "            output = model(image)\n",
    "            pred_label = torch.argmax(extract_relevant_logits(output), dim=1).item()\n",
    "\n",
    "            if pred_label == true_label:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total * 100 if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all asian-american/white neutral images accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asian american neutral image tensor shape: torch.Size([109, 3, 256, 256])\n",
      "White neutral image tensor shape: torch.Size([183, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "asian_american_neutral_images = torch.cat([nam_list, naf_list], dim=0)\n",
    "asian_american_neutral_labels = torch.cat([nam_labels, naf_labels])\n",
    "\n",
    "white_neutral_images = torch.cat([nwm_list, nwf_list], dim=0)\n",
    "white_neutral_labels = torch.cat([nwm_labels, nwf_labels])\n",
    "\n",
    "print(\"Asian american neutral image tensor shape:\", asian_american_neutral_images.shape)\n",
    "print(\"White neutral image tensor shape:\", white_neutral_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy asian american neutral imgs: 90.83%\n",
      "Accuracy white neutral imgs: 93.99%\n"
     ]
    }
   ],
   "source": [
    "asian_american_neutral_accuracy = calculate_accuracy(asian_american_neutral_images, asian_american_neutral_labels, model)\n",
    "white_neutral_accuracy = calculate_accuracy(white_neutral_images, white_neutral_labels, model)\n",
    "\n",
    "print(f\"Accuracy asian american neutral imgs: {asian_american_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white neutral imgs: {white_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for neutral male aam-w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy asian american male neutral imgs: 94.23%\n",
      "Accuracy white male neutral imgs: 96.77%\n"
     ]
    }
   ],
   "source": [
    "asian_american_male_neutral_accuracy = calculate_accuracy(nam_list, nam_labels, model)\n",
    "white_male_neutral_accuracy = calculate_accuracy(nwm_list, nwm_labels, model)\n",
    "\n",
    "print(f\"Accuracy asian american male neutral imgs: {asian_american_male_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white male neutral imgs: {white_male_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy for neutral female aam-w images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy asian american female neutral imgs: 87.72%\n",
      "Accuracy white female neutral imgs: 91.11%\n"
     ]
    }
   ],
   "source": [
    "asian_american_female_neutral_accuracy = calculate_accuracy(naf_list, naf_labels, model)\n",
    "white_female_neutral_accuracy = calculate_accuracy(nwf_list, nwf_labels, model)\n",
    "\n",
    "print(f\"Accuracy asian american female neutral imgs: {asian_american_female_neutral_accuracy:.2f}%\")\n",
    "print(f\"Accuracy white female neutral imgs: {white_female_neutral_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list=[asian_american_neutral_accuracy,white_neutral_accuracy,asian_american_male_neutral_accuracy,white_male_neutral_accuracy,asian_american_female_neutral_accuracy,white_female_neutral_accuracy]\n",
    "col_list = [\n",
    "    \"asian_american_neutral_accuracy\", \"white_neutral_accuracy\", \"asian_american_male_neutral_accuracy\", \"white_male_neutral_accuracy\",\n",
    "    \"asian_american_female_neutral_accuracy\", \"white_female_neutral_accuracy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_accuracy = [round(value, 3) for value in accuracy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian_american_neutral_accuracy</th>\n",
       "      <td>90.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_neutral_accuracy</th>\n",
       "      <td>93.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian_american_male_neutral_accuracy</th>\n",
       "      <td>94.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_male_neutral_accuracy</th>\n",
       "      <td>96.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian_american_female_neutral_accuracy</th>\n",
       "      <td>87.719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        accuracy(%)\n",
       "asian_american_neutral_accuracy              90.826\n",
       "white_neutral_accuracy                       93.989\n",
       "asian_american_male_neutral_accuracy         94.231\n",
       "white_male_neutral_accuracy                  96.774\n",
       "asian_american_female_neutral_accuracy       87.719"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(rounded_accuracy,col_list,columns=['accuracy(%)'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('metrics_aam-w_onlyneutral.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAIRNESS METRICS PRE-SUPPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def get_preds_in_batches_reduced(model, images, batch_size=32, device=\"cuda\"):\n",
    "    dataset = TensorDataset(images)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    preds = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "            logits = extract_relevant_logits(output)\n",
    "            pred = logits.argmax(dim=1).cpu()\n",
    "            preds.append(pred)\n",
    "\n",
    "    return torch.cat(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_metrics(preds, labels, target_class):\n",
    "    # True Positive Rate (TPR): correctly predicted as class y / all actual class y\n",
    "    # False Positive Rate (FPR): predicted as class y but actual not y / all actual not y\n",
    "    true_positive = ((preds == target_class) & (labels == target_class)).sum().item()\n",
    "    false_positive = ((preds == target_class) & (labels != target_class)).sum().item()\n",
    "    actual_positive = (labels == target_class).sum().item()\n",
    "    actual_negative = (labels != target_class).sum().item()\n",
    "\n",
    "    tpr = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    fpr = false_positive / actual_negative if actual_negative > 0 else 0\n",
    "\n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_equalized_odds_gap(model, asian_american_images, asian_american_labels, white_images, white_labels, target_class, device=\"cuda\"):\n",
    "    preds_b = get_preds_in_batches_reduced(model, asian_american_images, device=device)\n",
    "    preds_w = get_preds_in_batches_reduced(model, white_images, device=device)\n",
    "\n",
    "    tpr_b, fpr_b = compute_group_metrics(preds_b, asian_american_labels, target_class)\n",
    "    tpr_w, fpr_w = compute_group_metrics(preds_w, white_labels, target_class)\n",
    "\n",
    "    gap = abs(tpr_b - tpr_w) + abs(fpr_b - fpr_w)\n",
    "    return gap, tpr_b,tpr_w,fpr_b,fpr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized Odds Gap (Neutral): 0.032\n"
     ]
    }
   ],
   "source": [
    "target_class = 0 #for neutral, which is index 0 in our modified emotion set\n",
    "gap1,tpr_aam1,tpr_w1,fpr_aam1,fpr_w1 = compute_equalized_odds_gap(model, asian_american_images, asian_american_labels, white_images, white_labels, target_class)\n",
    "print(f\"Equalized Odds Gap (Neutral): {gap1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR Asian american N: 0.908, TPR White N: 0.940\n",
      "FPR Asian american N: 0.000, FPR White N: 0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"TPR Asian american N: {tpr_aam1:.3f}, TPR White N: {tpr_w1:.3f}\")\n",
    "print(f\"FPR Asian american N: {fpr_aam1:.3f}, FPR White N: {fpr_w1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as observed above, the TPR for aam-N and white neutral images is almost similar and FPR is 0 for both these classes. so no bias can be indicated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
